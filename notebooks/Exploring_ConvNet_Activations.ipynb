{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring ConvNet Activations\n",
    "\n",
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/covid_illegitimate_features/blob/main/notebooks/Exploring_ConvNet_Activations.ipynb)"
   ],
   "id": "8b5a142e-b18e-44c9-b180-d0b0aa29ec5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if running on Google Colab\n",
    "#!git clone https://github.com/shaivimalik/covid_illegitimate_features.git\n",
    "#!pip install -r covid_illegitimate_features/requirements.txt\n",
    "#%cd covid_illegitimate_features/notebooks"
   ],
   "id": "54f85166-0120-4881-9c1c-3b5adf6891aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow import data as tf_data"
   ],
   "id": "72cb7eda-1335-4dde-972f-ee8fa2b3f1fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size\n",
    "image_size = (256,256)\n",
    "batch_size = 4\n",
    "\n",
    "# Load the dataset from directory\n",
    "dataset_leak = keras.utils.image_dataset_from_directory(\n",
    "    '../different_backgrounds', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Split the dataset into train, validation, and test sets (70-10-20)\n",
    "train_ds_leak = dataset_leak.take(tf_data.experimental.cardinality(dataset_leak).numpy()*0.7)\n",
    "remaining_ds_leak = dataset_leak.skip(tf_data.experimental.cardinality(dataset_leak).numpy()*0.7)\n",
    "val_ds_leak = remaining_ds_leak.take(tf_data.experimental.cardinality(dataset_leak).numpy()*0.1)\n",
    "test_ds_leak = remaining_ds_leak.skip(tf_data.experimental.cardinality(dataset_leak).numpy()*0.1)"
   ],
   "id": "d1205916-aa55-4084-88f0-4ddf808a891c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation layers\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "]\n",
    "# Function to apply data augmentation\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images\n",
    "\n",
    "# Apply data augmentation to training dataset\n",
    "train_ds_leak = train_ds_leak.map(lambda img, label: (data_augmentation(img), label),num_parallel_calls=tf_data.AUTOTUNE)\n",
    "# Prefetch test and validation datasets for performance\n",
    "test_ds_leak = test_ds_leak.prefetch(tf_data.AUTOTUNE)\n",
    "val_ds_leak = val_ds_leak.prefetch(tf_data.AUTOTUNE)"
   ],
   "id": "87ab1c3d-75ec-45e9-bc3e-1c31d5e4063e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "# Create model\n",
    "model_leak = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model_leak.add(keras.Input(shape=image_size + (3,)))\n",
    "\n",
    "# Add rescaling layer to normalize pixel values\n",
    "model_leak.add(layers.Rescaling(scale=1./255))\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model_leak.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model_leak.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model_leak.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model_leak.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model_leak.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model_leak.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "\n",
    "# Flatten the output and add dense layers\n",
    "model_leak.add(layers.Flatten())\n",
    "model_leak.add(layers.Dense(64, activation='relu'))\n",
    "model_leak.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model_leak.summary()"
   ],
   "id": "d3d61bcc-e347-475e-a3a9-006f8fa8aed4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Compile the model\n",
    "model_leak.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history_leak = model_leak.fit(train_ds_leak, batch_size=batch_size, epochs=epochs, validation_data=val_ds_leak)"
   ],
   "id": "52346e81-121d-4ab6-95ac-479a3c3e1518"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_leak.history['accuracy'])\n",
    "plt.plot(history_leak.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history_leak.history['loss'])\n",
    "plt.plot(history_leak.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "523e8c43-c238-469c-a8f0-dd90419a0aab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "score = model_leak.evaluate(test_ds_leak)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ],
   "id": "aef7fad7-96f0-4093-a54a-1590aefcbc0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "\n",
    "# Define image titles for visualization\n",
    "image_titles = ['husky', 'wolf']\n",
    "\n",
    "# Create lists of file paths for husky and wolf images\n",
    "husky_files = np.array(['../different_backgrounds/husky/'+x for x in os.listdir('../different_backgrounds/husky')])\n",
    "wolf_files = np.array(['../different_backgrounds/wolf/'+x for x in os.listdir('../different_backgrounds/wolf')])\n",
    "\n",
    "# Load random images for each class and convert them to a Numpy array\n",
    "husky = keras.utils.load_img(np.random.choice(husky_files), target_size=image_size)\n",
    "wolf = keras.utils.load_img(np.random.choice(wolf_files), target_size=image_size)\n",
    "images = np.asarray([np.array(husky), np.array(wolf)])\n",
    "X = np.array([keras.utils.img_to_array(img) for img in images])\n",
    "\n",
    "# Render the original images\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define a function to modify the model for GradCAM\n",
    "def model_modifier_function(cloned_model):\n",
    "    cloned_model.layers[-1].activation = keras.activations.linear\n",
    "\n",
    "# Define a score function for GradCAM\n",
    "def score_function(output):\n",
    "    return (output[0,0], output[1,1])\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model_leak, model_modifier=model_modifier_function, clone=True)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score_function, X)\n",
    "\n",
    "# Render the images with GradCAM heatmaps overlaid\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e2b0cafe-ebd8-408f-8743-ddd337fec415"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "background_swap = keras.utils.image_dataset_from_directory(\n",
    "    '../background_swap', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "score = model_leak.evaluate(background_swap)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ],
   "id": "07d2fb35-b304-4bbf-bc5b-f734a4dd8ac1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size\n",
    "image_size = (256,256)\n",
    "batch_size = 4\n",
    "\n",
    "# Load the dataset from directory\n",
    "dataset = keras.utils.image_dataset_from_directory(\n",
    "    '../same_backgrounds', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Split the dataset into train, validation, and test sets (70-10-20)\n",
    "train_ds = dataset.take(tf_data.experimental.cardinality(dataset).numpy()*0.7)\n",
    "remaining_ds = dataset.skip(tf_data.experimental.cardinality(dataset).numpy()*0.7)\n",
    "val_ds = remaining_ds.take(tf_data.experimental.cardinality(dataset).numpy()*0.1)\n",
    "test_ds = remaining_ds.skip(tf_data.experimental.cardinality(dataset).numpy()*0.1)"
   ],
   "id": "9117797a-f131-4079-8769-8e11ff1dbc06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation layers\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images\n",
    "\n",
    "# Apply data augmentation to training dataset\n",
    "train_ds = train_ds.map(lambda img, label: (data_augmentation(img), label),num_parallel_calls=tf_data.AUTOTUNE)\n",
    "# Prefetch test and validation datasets for performance\n",
    "test_ds = test_ds.prefetch(tf_data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf_data.AUTOTUNE)"
   ],
   "id": "a8bd6208-34b0-48c8-adfc-32093822e94e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "# Create the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.Input(shape=image_size + (3,)))\n",
    "\n",
    "# Add rescaling layer to normalize pixel values\n",
    "model.add(layers.Rescaling(scale=1./255))\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "\n",
    "# Flatten the output and add dense layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ],
   "id": "67b08271-1937-4247-9c85-a6278d21d4df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_ds, batch_size=batch_size, epochs=epochs, validation_data=val_ds)"
   ],
   "id": "78265aec-bbf1-4523-bbd0-0a81a3f108f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "6857d7ef-88f0-413e-9694-9e58019e5320"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "score = model.evaluate(test_ds)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ],
   "id": "a2ac9c9b-f26a-4ac7-86b6-df14cc8e451c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "\n",
    "# Define image titles for visualization\n",
    "image_titles = ['husky', 'wolf']\n",
    "\n",
    "# Create lists of file paths for husky and wolf images\n",
    "husky_files = np.array(['../same_backgrounds/husky/'+x for x in os.listdir('../same_backgrounds/husky')])\n",
    "wolf_files = np.array(['../same_backgrounds/wolf/'+x for x in os.listdir('../same_backgrounds/wolf')])\n",
    "\n",
    "# Load random images for each class and convert them to a Numpy array\n",
    "husky = keras.utils.load_img(np.random.choice(husky_files), target_size=image_size)\n",
    "wolf = keras.utils.load_img(np.random.choice(wolf_files), target_size=image_size)\n",
    "images = np.asarray([np.array(husky), np.array(wolf)])\n",
    "X = np.array([keras.utils.img_to_array(img) for img in images])\n",
    "\n",
    "# Render the original images\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define a function to modify the model for GradCAM\n",
    "def model_modifier_function(cloned_model):\n",
    "    cloned_model.layers[-1].activation = keras.activations.linear\n",
    "\n",
    "# Define a score function for GradCAM\n",
    "def score_function(output):\n",
    "    return (output[0,0], output[1,1])\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model, model_modifier=model_modifier_function, clone=True)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score_function, X)\n",
    "\n",
    "# Render the images with GradCAM heatmaps overlaid\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b52f9167-f588-456c-a10f-cc11e884c392"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
