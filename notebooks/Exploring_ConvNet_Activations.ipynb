{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring ConvNet Activations\n",
    "\n",
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/covid_illegitimate_features/blob/main/notebooks/Exploring_ConvNet_Activations.ipynb)"
   ],
   "id": "7860cdf4-7a52-4dc1-97e3-d3dcd18138ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will continue our discussion of data leakage and\n",
    "its impact on model performance by training a convolutional neural\n",
    "network (CNN) to distinguish between husky dogs and wolves \\[2\\]. We\n",
    "will use small datasets of 100 images (50 of each class). This\n",
    "classification task will help us understand how a model can learn\n",
    "illegitimate features.\n",
    "\n",
    "We will follow two approaches:\n",
    "\n",
    "-   Train without Data Leakage: In this case, the dataset will have\n",
    "    husky images with grass backgrounds and wolf images with snow\n",
    "    backgrounds. We will train and evaluate our CNN on this dataset and\n",
    "    report the accuracy and confusion matrix obtained on the test set.\n",
    "\n",
    "-   Train without Data Leakage: In this case, images of both classes\n",
    "    have a white background. We will train and evaluate our CNN on this\n",
    "    dataset and report the accuracy and confusion matrix obtained on the\n",
    "    test set.\n",
    "\n",
    "Model Architecture (for both approaches):\n",
    "\n",
    "-   3 convolutional layers with ReLU activation, each followed by a\n",
    "    max-pooling layer\n",
    "-   Flatten layer\n",
    "-   2 fully connected layers\n",
    "    -   First layer with ReLU activation\n",
    "    -   Second layer with softmax activation\n",
    "\n",
    "Each model will be trained for 10 epochs using the Adam optimizer, with\n",
    "a learning rate of 0.001 and a batch size of 4 images."
   ],
   "id": "49ea48cf-513e-4276-abb7-7e8c28fcf84e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if running on Google Colab\n",
    "#!git clone https://github.com/shaivimalik/covid_illegitimate_features.git\n",
    "#!pip install -r covid_illegitimate_features/requirements.txt\n",
    "#%cd covid_illegitimate_features/notebooks"
   ],
   "id": "6db850c2-85f4-4504-9c5f-183526772d4b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Leakage\n",
    "\n",
    "Data leakage occurs when a model learns to recognise patterns or\n",
    "relationships between the features and target variable during training\n",
    "that don’t exist in the real-world data. Since these patterns won’t be\n",
    "present in the real-world data about which the claims are made, models\n",
    "with data leakage errors fail to generalise to unseen data \\[1\\]. Data\n",
    "leakage includes errors such as:\n",
    "\n",
    "-   **No test set:** If the model is trained and tested on the same\n",
    "    data, it will perform exceptionally well on the test set, but it\n",
    "    will fail on unseen data.\n",
    "\n",
    "-   **Temporal leakage:** This occurs when data from the future is used\n",
    "    to train a model created to predict future events.\n",
    "\n",
    "-   **Duplicates in datasets:** If there are duplicate data points in\n",
    "    both the training and test sets, the model can memorize these\n",
    "    instances, leading to inflated performance metrics.\n",
    "\n",
    "-   **Pre-processing on training and test set:** If pre-processing is\n",
    "    performed on the entire dataset, information about the test set may\n",
    "    leak into the training set.\n",
    "\n",
    "-   **Model uses features that are not legitimate:** If the model has\n",
    "    access to features that should not be legitimately available for\n",
    "    use. For example, when information about the target variable is\n",
    "    incorporated into the features used for training.\n",
    "\n",
    "Data leakage leads to overly optimistic estimates of model performance.\n",
    "It is also identified as the major cause behind the reproducibility\n",
    "crisis in ML-Based science\n",
    "[3](Nisbet,%20R.,%20Elder,%20J.,%20and%20Miner,%20G.%20Handbook%20of%20Statistical%20Analysis%20and%20Data%20Mining%20Applications.%20Elsevier,%202009.%20ISBN%20978-0-12-374765-5.).\n",
    "\n",
    "In this notebook, we will discover the consequences of Model uses\n",
    "features that are not legitimate on model performance."
   ],
   "id": "3d19be0a-33c4-46a2-b87e-142ffdb12db3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset with Different Backgrounds for Each Class\n",
    "\n",
    "In this section, we will load our dataset and split it into training,\n",
    "validation and test sets. The training set is used to train the model,\n",
    "the validation is used to find optimal hyperameter values (learning\n",
    "rate, epochs, batch size) and the test set is used to evaluate the\n",
    "classifier.\n",
    "\n",
    "We will divide the dataset into a 70-10-20 split: 70% of the images will\n",
    "be used for training, 10% of the images will be used for validation and\n",
    "20% will be used for testing. In this notebook, we will not perform any\n",
    "hyperparameter optimization, but feel free to experiment with the\n",
    "hyperparameters to find optimal values for each model using the\n",
    "validation set.\n",
    "\n",
    "We start by importing the required libraries."
   ],
   "id": "cb69b295-0a4d-48fd-849b-52b8eb15b7f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "keras.utils.set_random_seed(27)"
   ],
   "id": "5bef9380-8a21-47ac-8eb1-03c589e62504"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we load our dataset using `image_dataset_from_directory`\n",
    "function from Keras. The images are resized to (256,256) pixels and\n",
    "batches of 4 images are created. The label associated with each image is\n",
    "one-hot encoded."
   ],
   "id": "68cacb91-1b39-4ed7-b4fc-6cfcdda8ebf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size\n",
    "image_size = (256,256)\n",
    "batch_size = 4\n",
    "\n",
    "# Load training and validation sets from directory\n",
    "train_ds_leak, val_ds_leak= keras.utils.image_dataset_from_directory(\n",
    "    '../different_backgrounds/train', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size,\n",
    "    seed=27,\n",
    "    validation_split=0.125,\n",
    "    subset='both'\n",
    ")"
   ],
   "id": "97ada25e-b5cb-40f2-adde-0fd31072b719"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set from directory\n",
    "test_ds_leak= keras.utils.image_dataset_from_directory(\n",
    "    '../different_backgrounds/test', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size,\n",
    "    seed=27,\n",
    "    shuffle=False\n",
    ")"
   ],
   "id": "1be20100-6eeb-4256-bfd4-65a33b6b2db2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Model with Data Leakage\n",
    "\n",
    "In this section, we will train and evaluate our model. We will use the\n",
    "`Sequential` class from Keras to create the model. The model will\n",
    "consist of 3 convolutional layers, each followed by a max pooling layer.\n",
    "These will be followed by a flatten layer and two fully connected\n",
    "(dense) layers. All convolutional layers and the first dense layer will\n",
    "use ReLU activation. The final dense layer will use softmax activation,\n",
    "which is equivalent to sigmoid when doing binary classification."
   ],
   "id": "ef6af75b-5d47-4e33-8b68-4c6294bb65d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "# Create model\n",
    "model_leak = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model_leak.add(keras.Input(shape=image_size + (3,)))\n",
    "\n",
    "# Add rescaling layer to normalize pixel values\n",
    "model_leak.add(layers.Rescaling(scale=1./255))\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model_leak.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model_leak.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model_leak.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model_leak.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model_leak.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model_leak.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "\n",
    "# Flatten the output and add dense layers\n",
    "model_leak.add(layers.Flatten())\n",
    "model_leak.add(layers.Dense(64, activation='relu'))\n",
    "model_leak.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model_leak.summary()"
   ],
   "id": "bb1fadae-2a47-457f-a94f-edd8e5214579"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we compile and train our model. We use\n",
    "`categorical_crossentropy` loss and train our model on the training set\n",
    "for 10 epochs with `adam` optimizer."
   ],
   "id": "4bfd3f2d-5335-435e-8425-606184230be0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Compile the model\n",
    "model_leak.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history_leak = model_leak.fit(train_ds_leak, batch_size=batch_size, epochs=epochs, validation_data=val_ds_leak)"
   ],
   "id": "50976438-0cf5-4cda-a8de-b3bd8aa17eba"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll plot the accuracy and loss for both the training and\n",
    "validation sets across epochs. These plots are useful for identifying\n",
    "signs of overfitting."
   ],
   "id": "cdfc210a-5320-4210-b821-02064d94786c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_leak.history['accuracy'])\n",
    "plt.plot(history_leak.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_leak.history['loss'])\n",
    "plt.plot(history_leak.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "eee6ed52-4dae-4254-9e6a-9acb63d669ea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we evaluate our model on the test set, compute the accuracy and\n",
    "display the confusion matrix."
   ],
   "id": "53f64768-97db-4180-be38-dc7c7e69c029"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "score_leak = model_leak.evaluate(test_ds_leak)\n",
    "print(\"Test loss:\", score_leak[0])\n",
    "print(\"Test accuracy:\", score_leak[1])"
   ],
   "id": "d1bd3b1c-e34c-441e-90b8-d84d2952b22a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes for the test dataset using the 'model_leak'\n",
    "y_pred_leak = model_leak.predict(test_ds_leak)\n",
    "\n",
    "# Convert predicted probabilities to class labels (0 or 1)\n",
    "y_pred_leak = np.argmax(y_pred_leak, axis=1)\n",
    "\n",
    "# Extract true labels from the test dataset\n",
    "y_true_leak = np.concatenate([np.argmax(label, axis=1) for _, label in test_ds_leak], axis=0)\n",
    "\n",
    "# Create a confusion matrix comparing true labels to predicted labels\n",
    "conf_mat_leak = confusion_matrix(y_true_leak, y_pred_leak)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "ConfusionMatrixDisplay(conf_mat_leak, display_labels=['husky', 'wolf']).plot(cmap='Blues')"
   ],
   "id": "97d1d480-ad14-4c3e-b605-448fdb2548c1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s find the pixels responsible for an image being classified as a\n",
    "husky dog or wolf using GradCAM \\[4\\]. You may learn more about saliency\n",
    "maps and feature visualisation\n",
    "[here](https://harvard-iacs.github.io/2021-CS109B/lectures/lecture17/)."
   ],
   "id": "d0bb52f0-b87c-4bae-aa7e-e6a389e2e07c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "\n",
    "# Define image titles for visualization\n",
    "image_titles = ['husky', 'wolf']\n",
    "\n",
    "# Create lists of file paths for husky and wolf images\n",
    "husky_files = np.array(['../different_backgrounds/test/husky/'+x for x in os.listdir('../different_backgrounds/test/husky')])\n",
    "wolf_files = np.array(['../different_backgrounds/test/wolf/'+x for x in os.listdir('../different_backgrounds/test/wolf')])\n",
    "\n",
    "# Load random images for each class and convert them to a Numpy array\n",
    "husky = keras.utils.load_img(np.random.choice(husky_files), target_size=image_size)\n",
    "wolf = keras.utils.load_img(np.random.choice(wolf_files), target_size=image_size)\n",
    "images = np.asarray([np.array(husky), np.array(wolf)])\n",
    "X = np.array([keras.utils.img_to_array(img) for img in images])\n",
    "\n",
    "# Render the original images\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define a function to modify the model for GradCAM\n",
    "def model_modifier_function(cloned_model):\n",
    "    cloned_model.layers[-1].activation = keras.activations.linear\n",
    "\n",
    "# Define a score function for GradCAM\n",
    "def score_function(output):\n",
    "    return (output[0,0], output[1,1])\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model_leak, model_modifier=model_modifier_function, clone=True)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score_function, X)\n",
    "\n",
    "# Render the images with GradCAM heatmaps overlaid\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e521e6af-c6ef-492b-87ee-7a58b8c08275"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmaps reveal that background pixels are primarily responsible for\n",
    "the model’s classification decisions. Instead of learning to distinguish\n",
    "between wolves and huskies, the model has learned to differentiate\n",
    "between grass and snow backgrounds. We can further validate this\n",
    "observation by evaluating the classifier on a dataset where the\n",
    "backgrounds are swapped: huskies with snow backgrounds and wolves with\n",
    "green backgrounds. The next cell loads this background swapped dataset\n",
    "using `image_dataset_from_directory` and computes the accuracy obtained\n",
    "by the model."
   ],
   "id": "6e880dd5-c08f-48fd-891c-3895cc0cb6f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display wolf image with green background\n",
    "keras.utils.load_img(\"../background_swap/wolf/\"+np.random.choice(os.listdir(\"../background_swap/wolf\")), target_size=(256,256))"
   ],
   "id": "09567823-e5eb-4446-9bc9-e4208aedc895"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display husky image with snow background\n",
    "keras.utils.load_img(\"../background_swap/husky/\"+np.random.choice(os.listdir(\"../background_swap/husky\")), target_size=(256,256))"
   ],
   "id": "98958b5c-1e32-4c06-b804-87b651f2b2d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on background_swap dataset\n",
    "background_swap = keras.utils.image_dataset_from_directory(\n",
    "    '../background_swap', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=27\n",
    ")\n",
    "score_swap = model_leak.evaluate(background_swap)\n",
    "print(\"Test loss:\", score_swap[0])\n",
    "print(\"Test accuracy:\", score_swap[1])"
   ],
   "id": "09844c84-2e30-4965-8a88-5a38fe3f6e90"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s performance on this dataset is extremely poor. In the next\n",
    "cell, we display the confusion matrix obtained on this dataset."
   ],
   "id": "427dd998-e762-4dbf-a84b-f91d2062bf95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes for the background-swapped dataset using the 'model_leak'\n",
    "y_pred_swap = model_leak.predict(background_swap)\n",
    "\n",
    "# Convert predicted probabilities to class labels (0 or 1)\n",
    "y_pred_swap = np.argmax(y_pred_swap, axis=1)\n",
    "\n",
    "# Extract true labels from the background-swapped dataset\n",
    "y_true_swap = np.concatenate([np.argmax(label, axis=1) for _, label in background_swap], axis=0)\n",
    "\n",
    "# Create a confusion matrix comparing true labels to predicted labels\n",
    "conf_mat_swap = confusion_matrix(y_true_swap, y_pred_swap)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "ConfusionMatrixDisplay(conf_mat_swap, display_labels=['husky', 'wolf']).plot(cmap='Blues')"
   ],
   "id": "9477da01-ab12-4b86-818b-459043cc3778"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, we can conclude that the model has\n",
    "misclassified wolves as huskies and vice versa. This indicates that the\n",
    "model has learned features that recognize the snow and grass\n",
    "backgrounds."
   ],
   "id": "56d275dd-6df7-43be-845d-ae178d96a039"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset with White Backgrounds Across All Classes\n",
    "\n",
    "In this section, we will load a dataset consisting of images with a\n",
    "white background. The dataset consists of 100 images, split into 70 for\n",
    "training, 10 for validation, and 20 for testing.\n",
    "`image_dataset_from_directory` from Keras is used to load the dataset\n",
    "with `batch_size` of 4 and `image_size` of (256,256)."
   ],
   "id": "0935d45e-c042-495b-9fc0-43ec2f47b88a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size\n",
    "image_size = (256,256)\n",
    "batch_size = 4\n",
    "\n",
    "# Load training and validation sets from directory\n",
    "train_ds, val_ds = keras.utils.image_dataset_from_directory(\n",
    "    '../same_backgrounds/train', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size,\n",
    "    seed=27,\n",
    "    validation_split=0.125,\n",
    "    subset='both'\n",
    ")"
   ],
   "id": "e8b7e5a5-30c7-4247-a41c-6b44844791f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set from directory\n",
    "test_ds= keras.utils.image_dataset_from_directory(\n",
    "    '../same_backgrounds/test', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size,\n",
    "    seed=17,\n",
    "    shuffle=False\n",
    ")"
   ],
   "id": "4161f205-a7ae-4cc6-beac-7bffa4e6fe6d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating model without data leakage\n",
    "\n",
    "In this section, we will train and evaluate a new model, having no data\n",
    "leakage errors. We’ll create a model with the same architecture as\n",
    "before, using the `Sequential` class from Keras."
   ],
   "id": "6d7c8d3c-804b-47da-804f-b3a5e7486ddb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "# Create the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.Input(shape=image_size + (3,)))\n",
    "\n",
    "# Add rescaling layer to normalize pixel values\n",
    "model.add(layers.Rescaling(scale=1./255))\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "\n",
    "# Flatten the output and add dense layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ],
   "id": "547ef320-cb4a-479f-8fd3-c3ca7d48395e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile and train the model using a learning rate of 0.001, the\n",
    "`adam` optimizer, and `categorical_crossentropy` loss for 10 epochs.\n",
    "\n",
    "*Use different hyperparameter values and see how it affects performance\n",
    "on validation set.*"
   ],
   "id": "485dcf65-ecc1-4cb9-b665-a36c3db82c9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_ds, batch_size=batch_size, epochs=epochs, validation_data=val_ds)"
   ],
   "id": "1581a139-6a12-4e9c-8172-6e09102818ca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot loss and accuracy on the test and validation sets against the\n",
    "number of epochs."
   ],
   "id": "6f91b40c-661f-4f1a-990a-e72240260e79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "9259e202-7936-4aa3-9825-f62545ec9577"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the model’s accuracy on the test set and display the\n",
    "confusion matrix."
   ],
   "id": "75669f3b-544e-4b00-8fa0-88a89d871ab4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "score = model.evaluate(test_ds)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ],
   "id": "5f042b37-f01f-49d5-94bb-96fd9ee0a02a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes for the test dataset using the 'model'\n",
    "y_pred = model.predict(test_ds)\n",
    "\n",
    "# Convert predicted probabilities to class labels (0 or 1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Extract true labels from the test dataset\n",
    "y_true = np.concatenate([np.argmax(label, axis=1) for _, label in test_ds], axis=0)\n",
    "\n",
    "# Create a confusion matrix comparing true labels to predicted labels\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "ConfusionMatrixDisplay(conf_mat, display_labels=['husky', 'wolf']).plot(cmap='Blues')"
   ],
   "id": "1cb64b03-e31a-408b-bca7-869d3c6267ca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualise which pixels are responsible for image I being\n",
    "classified as an image of class C using GradCAM \\[4\\]."
   ],
   "id": "200eaee8-6928-48b7-84ac-6d74ba84b099"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "\n",
    "# Define image titles for visualization\n",
    "image_titles = ['husky', 'wolf']\n",
    "\n",
    "# Create lists of file paths for husky and wolf images\n",
    "husky_files = np.array(['../same_backgrounds/test/husky/'+x for x in os.listdir('../same_backgrounds/test/husky')])\n",
    "wolf_files = np.array(['../same_backgrounds/test/wolf/'+x for x in os.listdir('../same_backgrounds/test/wolf')])\n",
    "\n",
    "# Load random images for each class and convert them to a Numpy array\n",
    "husky = keras.utils.load_img(np.random.choice(husky_files), target_size=image_size)\n",
    "wolf = keras.utils.load_img(np.random.choice(wolf_files), target_size=image_size)\n",
    "images = np.asarray([np.array(husky), np.array(wolf)])\n",
    "X = np.array([keras.utils.img_to_array(img) for img in images])\n",
    "\n",
    "# Render the original images\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define a function to modify the model for GradCAM\n",
    "def model_modifier_function(cloned_model):\n",
    "    cloned_model.layers[-1].activation = keras.activations.linear\n",
    "\n",
    "# Define a score function for GradCAM\n",
    "def score_function(output):\n",
    "    return (output[0,0], output[1,1])\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model, model_modifier=model_modifier_function, clone=True)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score_function, X)\n",
    "\n",
    "# Render the images with GradCAM heatmaps overlaid\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8e3e36d9-b170-4c38-8da5-e73d1eac213e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "|  Metric  | With Data Leakage | Without Data Leakage |\n",
    "|:--------:|:-----------------:|:--------------------:|\n",
    "| Accuracy |       90.0        |         70.0         |\n",
    "\n",
    "In the case with data leakage, we achieved an accuracy of 90%, while in\n",
    "the case without data leakage, we achieved an accuracy of 70%. This\n",
    "indicates that data leakage led to overly optimistic measures of the\n",
    "model’s performance.\n",
    "\n",
    "The model should not have access to any information about the target\n",
    "variable, nor should it be allowed to learn features that are not\n",
    "legitimate. Determining which features are legitimate or illegitimate\n",
    "requires domain expertise. However, it is always good practice to\n",
    "examine the weights learned by a machine learning model. This can be\n",
    "done using saliency maps for complex models or by simply inspecting the\n",
    "array of weights for simpler machine learning models."
   ],
   "id": "142efcd6-2806-45d5-918c-2b15dd68a942"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1\\]: Kapoor S, Narayanan A. Leakage and the reproducibility crisis in\n",
    "machine-learning-based science. Patterns (N Y). 2023 Aug 4;4(9):100804.\n",
    "doi: 10.1016/j.patter.2023.100804. PMID: 37720327; PMCID: PMC10499856.\n",
    "\n",
    "\\[2\\]: Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016.\n",
    "“Why Should I Trust You?”: Explaining the Predictions of Any Classifier.\n",
    "In Proceedings of the 22nd ACM SIGKDD International Conference on\n",
    "Knowledge Discovery and Data Mining (KDD ’16). Association for Computing\n",
    "Machinery, New York, NY, USA, 1135–1144.\n",
    "https://doi.org/10.1145/2939672.2939778\n",
    "\n",
    "\\[4\\]: R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh and\n",
    "D. Batra, “Grad-CAM: Visual Explanations from Deep Networks via\n",
    "Gradient-Based Localization,” 2017 IEEE International Conference on\n",
    "Computer Vision (ICCV), Venice, Italy, 2017, pp. 618-626, doi:\n",
    "10.1109/ICCV.2017.74."
   ],
   "id": "555878a2-cf9c-4790-9e96-0e26379770a7"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
