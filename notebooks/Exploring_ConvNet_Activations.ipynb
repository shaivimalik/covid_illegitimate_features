{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring ConvNet Activations\n",
    "\n",
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/covid_illegitimate_features/blob/main/notebooks/Exploring_ConvNet_Activations.ipynb)"
   ],
   "id": "f419dc64-81c8-4dc2-9b6a-d6a50d86b4d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if running on Google Colab\n",
    "#!git clone https://github.com/shaivimalik/covid_illegitimate_features.git\n",
    "#!pip install -r covid_illegitimate_features/requirements.txt\n",
    "#%cd covid_illegitimate_features/notebooks"
   ],
   "id": "d598118a-c7cf-4d83-bc8b-58647b96ee13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow import data as tf_data"
   ],
   "id": "2c6aea05-d970-4118-8b18-9a229edc1910"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size\n",
    "image_size = (256,256)\n",
    "batch_size = 4\n",
    "\n",
    "# Load the dataset from directory\n",
    "dataset_leak = keras.utils.image_dataset_from_directory(\n",
    "    '../different_backgrounds', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Split the dataset into train, validation, and test sets (70-10-20)\n",
    "train_ds_leak = dataset_leak.take(tf_data.experimental.cardinality(dataset_leak).numpy()*0.7)\n",
    "remaining_ds_leak = dataset_leak.skip(tf_data.experimental.cardinality(dataset_leak).numpy()*0.7)\n",
    "val_ds_leak = remaining_ds_leak.take(tf_data.experimental.cardinality(dataset_leak).numpy()*0.1)\n",
    "test_ds_leak = remaining_ds_leak.skip(tf_data.experimental.cardinality(dataset_leak).numpy()*0.2)"
   ],
   "id": "af6141d0-0b85-40da-b016-3fbdf8aa4b41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation layers\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "]\n",
    "# Function to apply data augmentation\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images\n",
    "\n",
    "# Apply data augmentation to training dataset\n",
    "train_ds_leak = train_ds_leak.map(lambda img, label: (data_augmentation(img), label),num_parallel_calls=tf_data.AUTOTUNE)\n",
    "# Prefetch test and validation datasets for performance\n",
    "test_ds_leak = test_ds_leak.prefetch(tf_data.AUTOTUNE)\n",
    "val_ds_leak = val_ds_leak.prefetch(tf_data.AUTOTUNE)"
   ],
   "id": "a2c3bcd6-8daa-4f13-8d3d-7df3be59a74e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "# Create model\n",
    "model_leak = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model_leak.add(keras.Input(shape=image_size + (3,)))\n",
    "\n",
    "# Add rescaling layer to normalize pixel values\n",
    "model_leak.add(layers.Rescaling(scale=1./255))\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model_leak.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model_leak.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model_leak.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model_leak.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model_leak.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model_leak.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "\n",
    "# Flatten the output and add dense layers\n",
    "model_leak.add(layers.Flatten())\n",
    "model_leak.add(layers.Dense(64, activation='relu'))\n",
    "model_leak.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model_leak.summary()"
   ],
   "id": "fe4869b5-0f33-42de-8e33-df87cc2f619f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Compile the model\n",
    "model_leak.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history_leak = model_leak.fit(train_ds_leak, batch_size=batch_size, epochs=epochs, validation_data=val_ds_leak)"
   ],
   "id": "9dbff7dd-0ce3-4c4e-8dfe-395553e63510"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_leak.history['accuracy'])\n",
    "plt.plot(history_leak.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history_leak.history['loss'])\n",
    "plt.plot(history_leak.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "62e29d03-e794-4c11-b9c4-e32590ebe311"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "score = model_leak.evaluate(test_ds_leak)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ],
   "id": "d8bb6134-52fe-4153-8e07-c3454bee9e16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "\n",
    "# Define image titles for visualization\n",
    "image_titles = ['husky', 'wolf']\n",
    "\n",
    "# Create lists of file paths for husky and wolf images\n",
    "husky_files = np.array(['../different_backgrounds/husky/'+x for x in os.listdir('../different_backgrounds/husky')])\n",
    "wolf_files = np.array(['../different_backgrounds/wolf/'+x for x in os.listdir('../different_backgrounds/wolf')])\n",
    "\n",
    "# Load random images for each class and convert them to a Numpy array\n",
    "husky = keras.utils.load_img(np.random.choice(husky_files), target_size=image_size)\n",
    "wolf = keras.utils.load_img(np.random.choice(wolf_files), target_size=image_size)\n",
    "images = np.asarray([np.array(husky), np.array(wolf)])\n",
    "X = np.array([keras.utils.img_to_array(img) for img in images])\n",
    "\n",
    "# Render the original images\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define a function to modify the model for GradCAM\n",
    "def model_modifier_function(cloned_model):\n",
    "    cloned_model.layers[-1].activation = keras.activations.linear\n",
    "\n",
    "# Define a score function for GradCAM\n",
    "def score_function(output):\n",
    "    return (output[0,0], output[1,1])\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model_leak, model_modifier=model_modifier_function, clone=True)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score_function, X)\n",
    "\n",
    "# Render the images with GradCAM heatmaps overlaid\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "53b302a4-5906-4eb3-a0d0-23dbb1c0d91e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "background_swap = keras.utils.image_dataset_from_directory(\n",
    "    '../background_swap', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "score = model_leak.evaluate(background_swap)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ],
   "id": "45d0444f-04f2-4238-aac9-8d56af2ca30b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size\n",
    "image_size = (256,256)\n",
    "batch_size = 4\n",
    "\n",
    "# Load the dataset from directory\n",
    "dataset = keras.utils.image_dataset_from_directory(\n",
    "    '../same_backgrounds', \n",
    "    label_mode=\"categorical\", \n",
    "    image_size=image_size, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Split the dataset into train, validation, and test sets (70-10-20)\n",
    "train_ds = dataset.take(tf_data.experimental.cardinality(dataset).numpy()*0.7)\n",
    "remaining_ds = dataset.skip(tf_data.experimental.cardinality(dataset).numpy()*0.7)\n",
    "val_ds = remaining_ds.take(tf_data.experimental.cardinality(dataset).numpy()*0.1)\n",
    "test_ds = remaining_ds.skip(tf_data.experimental.cardinality(dataset).numpy()*0.2)"
   ],
   "id": "9f3309c7-9d24-4afc-8348-de77cae2bef5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation layers\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images\n",
    "\n",
    "# Apply data augmentation to training dataset\n",
    "train_ds = train_ds.map(lambda img, label: (data_augmentation(img), label),num_parallel_calls=tf_data.AUTOTUNE)\n",
    "# Prefetch test and validation datasets for performance\n",
    "test_ds = test_ds.prefetch(tf_data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf_data.AUTOTUNE)"
   ],
   "id": "e43900ac-bda0-42b4-88f0-41cc5e07331a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "# Create the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.Input(shape=image_size + (3,)))\n",
    "\n",
    "# Add rescaling layer to normalize pixel values\n",
    "model.add(layers.Rescaling(scale=1./255))\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"valid\", activation='relu', use_bias=True))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding=\"valid\"))\n",
    "\n",
    "# Flatten the output and add dense layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ],
   "id": "e4291a0d-2759-49ab-abab-52742fc64138"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_ds, batch_size=batch_size, epochs=epochs, validation_data=val_ds)"
   ],
   "id": "2c846c2e-2a29-4288-ac0b-125d6cbec54b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "9a127ddd-f4d4-4a47-bed6-4b9b84e77990"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "score = model.evaluate(test_ds)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ],
   "id": "6680af1b-7dd4-4284-86f1-b44f13746c0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "\n",
    "# Define image titles for visualization\n",
    "image_titles = ['husky', 'wolf']\n",
    "\n",
    "# Create lists of file paths for husky and wolf images\n",
    "husky_files = np.array(['../same_backgrounds/husky/'+x for x in os.listdir('../same_backgrounds/husky')])\n",
    "wolf_files = np.array(['../same_backgrounds/wolf/'+x for x in os.listdir('../same_backgrounds/wolf')])\n",
    "\n",
    "# Load random images for each class and convert them to a Numpy array\n",
    "husky = keras.utils.load_img(np.random.choice(husky_files), target_size=image_size)\n",
    "wolf = keras.utils.load_img(np.random.choice(wolf_files), target_size=image_size)\n",
    "images = np.asarray([np.array(husky), np.array(wolf)])\n",
    "X = np.array([keras.utils.img_to_array(img) for img in images])\n",
    "\n",
    "# Render the original images\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define a function to modify the model for GradCAM\n",
    "def model_modifier_function(cloned_model):\n",
    "    cloned_model.layers[-1].activation = keras.activations.linear\n",
    "\n",
    "# Define a score function for GradCAM\n",
    "def score_function(output):\n",
    "    return (output[0,0], output[1,1])\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model, model_modifier=model_modifier_function, clone=True)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score_function, X)\n",
    "\n",
    "# Render the images with GradCAM heatmaps overlaid\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "36532f1d-030d-419c-b0f9-01b6f15bb17e"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
