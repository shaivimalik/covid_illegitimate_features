{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing “Identification of COVID-19 samples from chest X-Ray images using deep learning: A comparison of transfer learning approaches” without Data Leakage\n",
    "\n",
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/covid_illegitimate_features/blob/main/notebooks/Correcting_Original_Result.ipynb)"
   ],
   "id": "13a0db86-d451-4902-be24-4168d15131f7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will reproduce the results published in\n",
    "**Identification of COVID-19 samples from chest X-Ray images using deep\n",
    "learning: A comparison of transfer learning approaches** \\[1\\] without\n",
    "data leakage. This study aims to recognize the chest X-ray images of\n",
    "COVID-19 cases from normal and pneumonia cases."
   ],
   "id": "ded251a4-37fb-4cd8-a012-d0ff3d3602aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if running on Google Colab\n",
    "#!git clone https://github.com/shaivimalik/covid_illegitimate_features.git\n",
    "#!pip install -r covid_illegitimate_features/requirements.txt\n",
    "#%cd covid_illegitimate_features/notebooks"
   ],
   "id": "73feac11-a92e-4024-981d-7c2f1363e92a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the datasets\n",
    "\n",
    "We will use two datasets:\n",
    "\n",
    "-   **COVID-19 Image Data Collection**\n",
    "    [2](COVID-19%20Image%20Data%20Collection:%20Prospective%20Predictions%20Are%20the%20Future%20Joseph%20Paul%20Cohen%20and%20Paul%20Morrison%20and%20Lan%20Dao%20and%20Karsten%20Roth%20and%20Tim%20Q%20Duong%20and%20Marzyeh%20Ghassemi%20arXiv:2006.11988,%202020)\n",
    "    is a public open dataset of chest X-ray and CT images of patients\n",
    "    which are positive or suspected of COVID-19 or other viral and\n",
    "    bacterial pneumonias (MERS, SARS, and ARDS.). The images in this\n",
    "    dataset were extracted from public databases, such as\n",
    "    Radiopaedia.org, the Italian Society of Medical and Interventional\n",
    "    Radiology, and Figure1.com, through manual collection and web\n",
    "    scraping. The database is regularly updating with new cases.\n",
    "\n",
    "-   **ChestX-ray8** \\[3\\] dataset comprises of 108,948 frontal-view\n",
    "    X-ray images (collected from the year of 1992 to 2015) of 32,717\n",
    "    unique patients with the text-mined eight common disease labels.\n",
    "\n",
    "The code cell below will download the datasets. Then, we will create\n",
    "TensorFlow Dataset objects and visualize chest X-ray images."
   ],
   "id": "aa6753e0-ae5d-4de5-be2c-2a4d3e19f7dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O images_01.tar.gz https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz\n",
    "!wget -O images_02.tar.gz https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz\n",
    "!wget -O images_03.tar.gz https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz\n",
    "!wget -O images_04.tar.gz https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz\n",
    "!wget -O images_05.tar.gz https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz\n",
    "!wget -O images_06.tar.gz https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz\n",
    "!wget -O images_07.tar.gz https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz\n",
    "!wget -O images_08.tar.gz https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz\n",
    "!wget -O images_09.tar.gz https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz\n",
    "!wget -O images_10.tar.gz https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz\n",
    "!wget -O images_11.tar.gz https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz\n",
    "!wget -O images_12.tar.gz https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz"
   ],
   "id": "c5e6e961-225b-4496-9599-a68b00dcab2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir chest_xray\n",
    "\n",
    "!gunzip images_01.tar.gz\n",
    "!gunzip images_02.tar.gz\n",
    "!gunzip images_03.tar.gz\n",
    "!gunzip images_04.tar.gz\n",
    "!gunzip images_05.tar.gz\n",
    "!gunzip images_06.tar.gz\n",
    "!gunzip images_07.tar.gz\n",
    "!gunzip images_08.tar.gz\n",
    "!gunzip images_09.tar.gz\n",
    "!gunzip images_10.tar.gz\n",
    "!gunzip images_11.tar.gz\n",
    "!gunzip images_12.tar.gz\n",
    "\n",
    "!tar -xvf images_01.tar -C chest_xray\n",
    "!tar -xvf images_02.tar -C chest_xray\n",
    "!tar -xvf images_03.tar -C chest_xray\n",
    "!tar -xvf images_04.tar -C chest_xray\n",
    "!tar -xvf images_05.tar -C chest_xray\n",
    "!tar -xvf images_06.tar -C chest_xray\n",
    "!tar -xvf images_07.tar -C chest_xray\n",
    "!tar -xvf images_08.tar -C chest_xray\n",
    "!tar -xvf images_09.tar -C chest_xray\n",
    "!tar -xvf images_10.tar -C chest_xray\n",
    "!tar -xvf images_11.tar -C chest_xray\n",
    "!tar -xvf images_12.tar -C chest_xray"
   ],
   "id": "0e2fa35f-7d58-4ecd-99d4-649188a4555f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ieee8023/covid-chestxray-dataset.git"
   ],
   "id": "9be062b6-8ce5-4b58-8564-d548f35c79fb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the required libraries."
   ],
   "id": "d4214c49-f6be-43c2-948c-b7e4bb93f28a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras_cv import layers as layers_cv\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(20)\n",
    "tf.random.set_seed(20)"
   ],
   "id": "fc3fcc1c-2161-41a6-9718-b6a56d8b0c7d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **COVID-19 Image Data Collection** and **ChestX-ray8** contain chest\n",
    "X-ray images of various lung diseases, so we need to filter and identify\n",
    "the COVID-19 images.\n",
    "\n",
    "Here, we extract the file paths of X-ray images from COVID-19 cases and\n",
    "remove duplicates."
   ],
   "id": "340f96e0-416c-4795-b5d9-05d906ea541f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_files = os.listdir('covid-chestxray-dataset/images')\n",
    "df = pd.read_csv('covid-chestxray-dataset/metadata.csv')\n",
    "df = df[df['finding']=='Pneumonia/Viral/COVID-19']\n",
    "df = df[df['modality']=='X-ray']\n",
    "df = df[df['view']!='L']\n",
    "df.drop_duplicates(subset='patientid', keep='first', inplace=True)\n",
    "covid_files = df['filename'].to_list()\n",
    "covid_paths = np.random.choice(covid_files, size=260)\n",
    "covid_paths = ['covid-chestxray-dataset/images/' + i for i in covid_paths]"
   ],
   "id": "fdf12a87-c44f-4706-a145-95d30097a6b0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the file paths of X-ray images for both Pneumonia and normal\n",
    "cases and remove any duplicates."
   ],
   "id": "e5a4acf7-2e1a-41bb-8fb6-9853b74b45ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data_Entry_2017_v2020.csv')\n",
    "normal_paths = df[df['Finding Labels']=='No Finding'].sample(300)\n",
    "normal_paths = normal_paths['Image Index'].to_list()\n",
    "normal_paths = ['chest_xray/images/' + i for i in normal_paths]\n",
    "pneumonia_paths = df[df['Finding Labels']=='Pneumonia'].sample(300)\n",
    "pneumonia_paths = pneumonia_paths['Image Index'].to_list()\n",
    "pneumonia_paths = ['chest_xray/images/' + i for i in pneumonia_paths]"
   ],
   "id": "6e20746b-e342-4d54-826c-188e1b0323cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of COVID-19 samples:\", len(covid_paths))\n",
    "print(\"Number of Normal samples:\", len(normal_paths))\n",
    "print(\"Number of Pneumonia samples:\", len(pneumonia_paths))"
   ],
   "id": "74206916-ff59-4647-a2ec-1b24c0c289dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the `from_tensor_slices` method to create `tf.data.Dataset`\n",
    "objects from the lists of paths."
   ],
   "id": "a2e83bcd-1046-47d0-a61c-ca1d254f5005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_ds = tf.data.Dataset.from_tensor_slices(covid_paths)\n",
    "normal_ds = tf.data.Dataset.from_tensor_slices(normal_paths)\n",
    "pneumonia_ds = tf.data.Dataset.from_tensor_slices(pneumonia_paths)"
   ],
   "id": "88981a0a-5eb8-4bc5-b414-5eaf3e976c8e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we assign labels to each image and use the `process_path` function\n",
    "to load and resize them to 224x224 pixels."
   ],
   "id": "24618e64-214c-4896-8868-20aacb6fdd71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path, label):\n",
    "  # Load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  # Convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.io.decode_jpeg(img, channels=3)\n",
    "  # Resize the image to the desired size\n",
    "  img = tf.image.resize(img, [224, 224])\n",
    "  return img, label\n",
    "\n",
    "labels = {\"covid-19\":0, \"normal\":1, \"pneumonia\":2}\n",
    "covid_ds = covid_ds.map(lambda x: process_path(x,labels['covid-19']))\n",
    "normal_ds = normal_ds.map(lambda x: process_path(x,labels['normal']))\n",
    "pneumonia_ds = pneumonia_ds.map(lambda x: process_path(x,labels['pneumonia']))"
   ],
   "id": "409c3909-3a57-4ecb-aadd-ccec670430a3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the chest X-ray images of each class."
   ],
   "id": "4219620d-e54d-46ef-b162-c8dece6ba572"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.load_img(covid_paths[5], color_mode='grayscale', target_size=(224,224))"
   ],
   "id": "e9091753-2edb-4736-b021-d6b7c51aa51b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.load_img(normal_paths[5], color_mode='grayscale', target_size=(224,224))"
   ],
   "id": "a9667276-04a2-4bd6-baa8-a5dd6f24ed0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.load_img(pneumonia_paths[5], color_mode='grayscale', target_size=(224,224))"
   ],
   "id": "2c341836-93a5-4048-954a-7bf13edd1a46"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the convolutional neural network(VGG19) via Transfer Learning\n",
    "\n",
    "In this section, we will train and evaluate our Convolutional Neural\n",
    "Network following the methodology outlined in the paper:\n",
    "\n",
    "-   Split the datasets into training, test, and validation sets.\n",
    "-   Apply the data augmentation strategy.\n",
    "-   Load the VGG19 model to extract features from the images.\n",
    "-   Add two dense layers with ReLU activation.\n",
    "-   Add a final dense layer with softmax activation.\n",
    "-   Train the model for 50 epochs with a learning rate of 0.001 using\n",
    "    the RMSprop optimizer.\n",
    "-   Use ReduceLROnPlateau to adjust the learning rate when validation\n",
    "    loss has stopped improving.\n",
    "-   Report the model’s accuracy, confusion matrix, and class-wise\n",
    "    precision, recall, and F1-score."
   ],
   "id": "4c5c9777-6c04-4884-bdc7-d0946b1c7fae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by splitting `covid_ds`, `normal_ds` and `pneumonia_ds`\n",
    "according to the statistics given in the paper. We then concatenate\n",
    "these splits to form training, test, and validation sets for model\n",
    "training and evaluation. `keras.applications.vgg19.preprocess_input`\n",
    "method is applied to preprocess the images, ensuring they are in the\n",
    "correct format required by the VGG19 model."
   ],
   "id": "9d4c91ef-a417-4718-9908-65d8223cbab3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting normal patients data acc. to stats given in paper\n",
    "normal_ds_train = normal_ds.take(200)\n",
    "normal_remaining = normal_ds.skip(200)\n",
    "normal_ds_val = normal_remaining.take(50)\n",
    "normal_ds_test = normal_remaining.skip(50)\n",
    "# Splitting pneumonia patients data acc. to stats given in paper\n",
    "pneumonia_ds_train = pneumonia_ds.take(200)\n",
    "pneumonia_remaining = pneumonia_ds.skip(200)\n",
    "pneumonia_ds_val = pneumonia_remaining.take(50)\n",
    "pneumonia_ds_test = pneumonia_remaining.skip(50)\n",
    "# Splitting covid patients data acc. to stats given in paper\n",
    "covid_ds_train = covid_ds.take(180)\n",
    "covid_remaining = covid_ds.skip(180)\n",
    "covid_ds_val = covid_remaining.take(40)\n",
    "covid_ds_test = covid_remaining.skip(40)"
   ],
   "id": "0bec9eef-3fb7-494f-b42a-9043c204f46b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "train_ds = (covid_ds_train.concatenate(normal_ds_train).concatenate(pneumonia_ds_train))\n",
    "validation_ds = (covid_ds_val.concatenate(normal_ds_val).concatenate(pneumonia_ds_val))\n",
    "test_ds = (covid_ds_test.concatenate(normal_ds_test).concatenate(pneumonia_ds_test))\n",
    "# Preprocess the images in each dataset using VGG19 preprocess_input\n",
    "train_ds = train_ds.map(lambda x, y: (keras.applications.vgg19.preprocess_input(x), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (keras.applications.vgg19.preprocess_input(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (keras.applications.vgg19.preprocess_input(x), y))"
   ],
   "id": "554c6753-e278-435d-bc18-8bdc3cf4855e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we create `RandomRotation` , `RandomTranslation`,\n",
    "`RandomShear`, `RandomZoom` and `RandomFlip` data augmentation layers\n",
    "and apply them on the training set."
   ],
   "id": "d189c6d1-1da0-468a-bede-14940f12edbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation layers\n",
    "augmentation_layers = [\n",
    "    layers.RandomRotation(0.2), # Randomly rotate images by up to 20 degrees\n",
    "    layers.RandomTranslation(0.1, 0.1), # Randomly translate images by up to 10% in x and y directions\n",
    "    layers_cv.RandomShear(x_factor=0.1, y_factor=0.1), # Randomly shear images by up to 10% in x and y directions\n",
    "    layers.RandomZoom(0.2), # Randomly zoom images by up to 20%\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\") # Randomly flip images horizontally and vertically\n",
    "]\n",
    "\n",
    "def data_augmentation(x):\n",
    "    for layer in augmentation_layers:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "# Apply data augmentation to training set\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))"
   ],
   "id": "fdd6496f-a5f6-45a4-80c0-a6f9a4f64753"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell batches the datasets (`train_ds`, `validation_ds`, and\n",
    "`test_ds`) into batches of 32 samples, uses prefetching to improve\n",
    "performance, and caches the datasets in memory for faster subsequent\n",
    "access."
   ],
   "id": "e0918975-c4bb-45fd-9212-762ea21eea8b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Configure datasets for performance\n",
    "train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()\n",
    "validation_ds = validation_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()\n",
    "test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()"
   ],
   "id": "8c6fa641-f79c-4338-83b4-19f91c564d36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the VGG19 model with weights pretrained on the\n",
    "ImageNet dataset. By setting `include_top=False`, we exclude the final\n",
    "classification layer of the VGG19 model. We set the `trainable`\n",
    "attribute of the VGG19 layers to `False`. Then, we add a `Flatten` layer\n",
    "to convert the VGG19 output into a one-dimensional vector. We follow\n",
    "this with two `Dense` layers with ReLU activation, having 1024 and 512\n",
    "neurons respectively. Finally, the output is fed into a `Dense` layer\n",
    "with a softmax activation function."
   ],
   "id": "3fa47ba9-7fee-430f-a1d0-17e25bc70d2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base model using VGG19 pre-trained on ImageNet\n",
    "base_model = keras.applications.VGG19(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet\n",
    "    input_shape=[224, 224, 3],  # Specify input shape\n",
    "    include_top=False,  # Do not include the ImageNet classifier at the top\n",
    ")\n",
    "\n",
    "# Freeze the base model to prevent its weights from being updated during training\n",
    "base_model.trainable = False\n",
    "\n",
    "# Flatten the output of VGG19\n",
    "x = layers.Flatten()(base_model.output)\n",
    "\n",
    "# Add Dense layer with 1024 neurons and ReLU activation\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Add Dense layer with 512 neurons and ReLU activation\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add final Dense layer with softmax activation for 3-class prediction\n",
    "predictions = layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Create the full model\n",
    "model = keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Display model summary, showing which layers are trainable\n",
    "model.summary(show_trainable=True)"
   ],
   "id": "c3daa998-4b53-486a-a4f6-02b23449a22c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on the training set using the RMSprop optimizer for\n",
    "50 epochs. We use `sparse_categorical_crossentropy` as our loss function\n",
    "since the labels are encoded as integers. We apply `ReduceLROnPlateau`\n",
    "to reduce the learning rate by a factor of 0.3 when the validation loss\n",
    "plateaus."
   ],
   "id": "5c271e9e-eec3-4988-ac94-919153ee29af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# Define learning rate reduction callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(factor=0.3)\n",
    "\n",
    "# Set number of training epochs\n",
    "epochs = 50\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=validation_ds, callbacks=[reduce_lr])"
   ],
   "id": "ebed7435-e17a-46a3-90eb-348aeed88296"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the training accuracy (`sparse_categorical_accuracy`) and\n",
    "validation accuracy (`val_sparse_categorical_accuracy`) against the\n",
    "number of epochs."
   ],
   "id": "b758762e-d09f-49c4-bc2e-1448212d6cf8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "aea547eb-f3c5-4f69-8265-b3140e01bcc0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we evaluate the model on the test set and report the accuracy."
   ],
   "id": "7d8d6ff9-0895-44cc-8af7-132d787de647"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print('Test loss :', loss)\n",
    "print('Test accuracy :', accuracy)"
   ],
   "id": "908df6c6-45fa-403a-a250-76612578fd24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cells display the true and predicted labels on the\n",
    "test set and generate a confusion matrix."
   ],
   "id": "d3228b8a-7b08-4f7e-99fc-26f5acccabae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = model.predict(test_ds)\n",
    "# Convert predicted probabilities to class labels by taking the index of the highest probability\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Predictions:\",y_pred)\n",
    "# Extract true labels from the test dataset\n",
    "y_true = tf.concat([label for _, label in test_ds], axis=0).numpy()\n",
    "print(\"True labels:\", y_true)"
   ],
   "id": "3ed1dc3e-ac7e-468d-b1d9-d6771a635971"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_mat = confusion_matrix(y_true,y_pred)\n",
    "# Display confusion matrix with labels\n",
    "ConfusionMatrixDisplay(conf_mat,display_labels=labels.keys()).plot(cmap='Blues')"
   ],
   "id": "d09c76c4-6254-4049-83ce-887ed31970fd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we report the class-wise precision, recall and f1-score of the\n",
    "model’s performance on the test set."
   ],
   "id": "32a35d22-a3d0-4803-a165-05a4ec7467e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# Print metrics for each class\n",
    "for key in labels.keys():\n",
    "  print(\"class:\", key)\n",
    "  print(\"Precision:\",report[str(labels[key])]['precision'])\n",
    "  print(\"Recall:\",report[str(labels[key])]['recall'])\n",
    "  print(\"F1-score:\",report[str(labels[key])]['f1-score'])\n",
    "  print()"
   ],
   "id": "dd30f6b7-afc1-43cc-b9f7-0c8ef84c6257"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s save the model for future inference tasks."
   ],
   "id": "2c9bcff5-8ab7-45af-9639-cf1099fd282b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('correct_covid.keras')"
   ],
   "id": "874ee012-df01-46d5-bfc2-15903698f9b4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use GradCAM \\[4\\] to identify the pixels responsible for an\n",
    "image being classified as normal, pneumonia, or COVID-19."
   ],
   "id": "6725a100-5e93-4343-91cf-732583791c19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "\n",
    "# Image titles for each class\n",
    "image_titles = ['Covid', 'Normal', 'Pneumonia']\n",
    "\n",
    "index = np.random.randint(-40, 0)\n",
    "\n",
    "# Load images and Convert them to a Numpy array\n",
    "covid = keras.utils.load_img(covid_paths[index], target_size=(224, 224))\n",
    "normal = keras.utils.load_img(normal_paths[index], target_size=(224, 224))\n",
    "pneumonia = keras.utils.load_img(pneumonia_paths[index], target_size=(224, 224))\n",
    "images = np.asarray([np.array(covid), np.array(normal), np.array(pneumonia)])\n",
    "\n",
    "X = np.array([tf.keras.utils.img_to_array(img) for img in images])\n",
    "\n",
    "# Rendering\n",
    "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to modify the model for GradCAM\n",
    "def model_modifier_function(cloned_model): \n",
    "  cloned_model.layers[-1].activation = tf.keras.activations.linear\n",
    "\n",
    "# Score function for GradCAM\n",
    "def score_function(output): return (output[0][0], output[1][1], output[2][2])\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model, model_modifier=model_modifier_function, clone=True)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score_function, X)\n",
    "\n",
    "# Rendering images with GradCAM heatmaps\n",
    "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "be49d42c-bea4-4921-b6f5-ad7caee1bbd7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "|  Metric  | Original | Reproduced | Reproduced without Data Leakage |\n",
    "|:--------:|:--------:|:----------:|:-------------------------------:|\n",
    "| Accuracy |   89.3   |   92.14    |              51.43              |\n",
    "\n",
    "In this notebook, we successfully reproduced the findings of\n",
    "**“Identification of COVID-19 Samples from Chest X-Ray Images Using Deep\n",
    "Learning: A Comparison of Transfer Learning Approaches”** without data\n",
    "leakage. The model’s accuracy dropped by 40%, confirming that it had\n",
    "previously relied on illegitimate features to distinguish between\n",
    "COVID-19, pneumonia, and normal patients when using the Chest X-Ray\n",
    "Images (Pneumonia) dataset from Kaggle. When working with medical data,\n",
    "it is crucial to consult a professional and ensure that no information\n",
    "is inadvertently leaked to the model—information that would not be\n",
    "available during real-world deployment."
   ],
   "id": "feced605-4932-4f40-8aff-d9ab92d40501"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1\\]: Rahaman, Md Mamunur et al. “Identification of COVID-19 samples\n",
    "from chest X-Ray images using deep learning: A comparison of transfer\n",
    "learning approaches.” Journal of X-ray science and technology vol. 28,5\n",
    "(2020): 821-839. doi:10.3233/XST-200715\n",
    "\n",
    "\\[3\\]: X. Wang, et al., “ChestX-Ray8: Hospital-Scale Chest X-Ray\n",
    "Database and Benchmarks on Weakly-Supervised Classification and\n",
    "Localization of Common Thorax Diseases,” in 2017 IEEE Conference on\n",
    "Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 2017\n",
    "pp. 3462-3471. doi: 10.1109/CVPR.2017.369\n",
    "\n",
    "\\[4\\]: R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh and\n",
    "D. Batra, “Grad-CAM: Visual Explanations from Deep Networks via\n",
    "Gradient-Based Localization,” 2017 IEEE International Conference on\n",
    "Computer Vision (ICCV), Venice, Italy, 2017, pp. 618-626, doi:\n",
    "10.1109/ICCV.2017.74."
   ],
   "id": "57d1699e-0584-4597-af40-d76d812d6e29"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
