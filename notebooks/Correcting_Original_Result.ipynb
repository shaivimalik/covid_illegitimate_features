{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing “Identification of COVID-19 samples from chest X-Ray images using deep learning: A comparison of transfer learning approaches” without Data Leakage\n",
    "\n",
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/covid_illegitimate_features/blob/main/notebooks/Correcting_Original_Result.ipynb)"
   ],
   "id": "cee5558e-dedf-47d3-8010-4069373023b9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will reproduce the results published in\n",
    "**Identification of COVID-19 samples from chest X-Ray images using deep\n",
    "learning: A comparison of transfer learning approaches** \\[1\\] without\n",
    "data leakage. This study aims to recognize the chest X-ray images of\n",
    "COVID-19 cases from normal and pneumonia cases."
   ],
   "id": "c4a41bd9-5762-477d-9430-0872dd10881b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if running on Google Colab\n",
    "#!git clone https://github.com/shaivimalik/covid_illegitimate_features.git\n",
    "#!pip install -r covid_illegitimate_features/requirements.txt\n",
    "#%cd covid_illegitimate_features/notebooks"
   ],
   "id": "7788e109-2fdb-4b5a-a605-cc6a368d9ad0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the datasets\n",
    "\n",
    "We will use two datasets:\n",
    "\n",
    "-   **COVID-19 Image Data Collection**\n",
    "    [2](COVID-19%20Image%20Data%20Collection:%20Prospective%20Predictions%20Are%20the%20Future%20Joseph%20Paul%20Cohen%20and%20Paul%20Morrison%20and%20Lan%20Dao%20and%20Karsten%20Roth%20and%20Tim%20Q%20Duong%20and%20Marzyeh%20Ghassemi%20arXiv:2006.11988,%202020)\n",
    "    is a public open dataset of chest X-ray and CT images of patients\n",
    "    which are positive or suspected of COVID-19 or other viral and\n",
    "    bacterial pneumonias (MERS, SARS, and ARDS.). The images in this\n",
    "    dataset were extracted from public databases, such as\n",
    "    Radiopaedia.org, the Italian Society of Medical and Interventional\n",
    "    Radiology, and Figure1.com, through manual collection and web\n",
    "    scraping. The database is regularly updating with new cases.\n",
    "\n",
    "-   **ChestX-ray8** \\[3\\] dataset comprises of 108,948 frontal-view\n",
    "    X-ray images of 32,717 (collected from the year of 1992 to 2015)\n",
    "    unique patients with the text-mined eight common disease labels.\n",
    "\n",
    "The code cell below will download the datasets. Then, we will create\n",
    "TensorFlow Dataset objects and visualize chest X-ray images."
   ],
   "id": "971f82e5-ff42-442b-b9de-f241a23b69b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O images_01.tar.gz https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz\n",
    "!wget -O images_02.tar.gz https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz\n",
    "!wget -O images_03.tar.gz https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz\n",
    "!wget -O images_04.tar.gz https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz\n",
    "!wget -O images_05.tar.gz https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz\n",
    "!wget -O images_06.tar.gz https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz\n",
    "!wget -O images_07.tar.gz https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz\n",
    "!wget -O images_08.tar.gz https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz\n",
    "!wget -O images_09.tar.gz https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz\n",
    "!wget -O images_10.tar.gz https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz\n",
    "!wget -O images_11.tar.gz https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz\n",
    "!wget -O images_12.tar.gz https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz"
   ],
   "id": "83a7afaa-8647-4a5a-9d01-8d4b0dba921f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir chest_xray\n",
    "\n",
    "!gunzip images_01.tar.gz\n",
    "!gunzip images_02.tar.gz\n",
    "!gunzip images_03.tar.gz\n",
    "!gunzip images_04.tar.gz\n",
    "!gunzip images_05.tar.gz\n",
    "!gunzip images_06.tar.gz\n",
    "!gunzip images_07.tar.gz\n",
    "!gunzip images_08.tar.gz\n",
    "!gunzip images_09.tar.gz\n",
    "!gunzip images_10.tar.gz\n",
    "!gunzip images_11.tar.gz\n",
    "!gunzip images_12.tar.gz\n",
    "\n",
    "!tar -xvf images_01.tar -C chest_xray\n",
    "!tar -xvf images_02.tar -C chest_xray\n",
    "!tar -xvf images_03.tar -C chest_xray\n",
    "!tar -xvf images_04.tar -C chest_xray\n",
    "!tar -xvf images_05.tar -C chest_xray\n",
    "!tar -xvf images_06.tar -C chest_xray\n",
    "!tar -xvf images_07.tar -C chest_xray\n",
    "!tar -xvf images_08.tar -C chest_xray\n",
    "!tar -xvf images_09.tar -C chest_xray\n",
    "!tar -xvf images_10.tar -C chest_xray\n",
    "!tar -xvf images_11.tar -C chest_xray\n",
    "!tar -xvf images_12.tar -C chest_xray"
   ],
   "id": "6bf402a4-6198-48bb-96d3-d175cc699e51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ieee8023/covid-chestxray-dataset.git"
   ],
   "id": "a9cf5328-a60c-4a17-bca7-f579f1a71f1a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the required libraries."
   ],
   "id": "d25600be-9a98-49ec-8300-5b7d52a3d063"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras_cv import layers as layers_cv\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(20)\n",
    "tf.random.set_seed(20)"
   ],
   "id": "868f1728-ccea-460b-a9c9-86901cf9bd21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **COVID-19 Image Data Collection** and **ChestX-ray8** contain chest\n",
    "X-ray images of various lung diseases, so we need to filter and identify\n",
    "the COVID-19 images.\n",
    "\n",
    "Here, we extract the file paths of X-ray images from COVID-19 cases and\n",
    "remove duplicates."
   ],
   "id": "670fe79b-4dc2-4799-92e3-00072e318367"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_files = os.listdir('covid-chestxray-dataset/images')\n",
    "df = pd.read_csv('covid-chestxray-dataset/metadata.csv')\n",
    "df = df[df['finding']=='Pneumonia/Viral/COVID-19']\n",
    "df = df[df['modality']=='X-ray']\n",
    "df = df[df['view']!='L']\n",
    "df.drop_duplicates(subset='patientid', keep='first', inplace=True)\n",
    "covid_files = df['filename'].to_list()\n",
    "covid_paths = np.random.choice(covid_files, size=260)\n",
    "covid_paths = ['covid-chestxray-dataset/images/' + i for i in covid_paths]"
   ],
   "id": "627efda6-cfc9-443e-b335-9639a245d04a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the file paths of X-ray images for both Pneumonia and normal\n",
    "cases and remove any duplicates."
   ],
   "id": "1cea6f5f-96f5-4f45-982f-55423e686272"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data_Entry_2017_v2020.csv')\n",
    "normal_paths = df[df['Finding Labels']=='No Finding'].sample(300)\n",
    "normal_paths = normal_paths['Image Index'].to_list()\n",
    "normal_paths = ['chest_xray/images/' + i for i in normal_paths]\n",
    "pneumonia_paths = df[df['Finding Labels']=='Pneumonia'].sample(300)\n",
    "pneumonia_paths = pneumonia_paths['Image Index'].to_list()\n",
    "pneumonia_paths = ['chest_xray/images/' + i for i in pneumonia_paths]"
   ],
   "id": "855ab699-be51-46cf-8b1d-6334817357c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of COVID-19 samples:\", len(covid_paths))\n",
    "print(\"Number of Normal samples:\", len(normal_paths))\n",
    "print(\"Number of Pneumonia samples:\", len(pneumonia_paths))"
   ],
   "id": "51f5da56-6558-4c9b-ac85-b687d3c68b78"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the `from_tensor_slices` method to create `tf.data.Dataset`\n",
    "objects from the lists of paths."
   ],
   "id": "fd9f4f9e-52f1-4f07-9762-04619683e3b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_ds = tf.data.Dataset.from_tensor_slices(covid_paths)\n",
    "normal_ds = tf.data.Dataset.from_tensor_slices(normal_paths)\n",
    "pneumonia_ds = tf.data.Dataset.from_tensor_slices(pneumonia_paths)"
   ],
   "id": "c694b5f8-06b8-4ff5-93aa-07c4ceee1ca0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we assign labels to each image and use the process_path function\n",
    "to load and resize them to 224x224 pixels."
   ],
   "id": "18a2d9df-1839-422a-9b4a-40d36e4f0e49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path, label):\n",
    "  # Load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  # Convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.io.decode_jpeg(img, channels=3)\n",
    "  # Resize the image to the desired size\n",
    "  img = tf.image.resize(img, [224, 224])\n",
    "  return img, label\n",
    "\n",
    "labels = {\"covid-19\":0, \"normal\":1, \"pneumonia\":2}\n",
    "covid_ds = covid_ds.map(lambda x: process_path(x,labels['covid-19']))\n",
    "normal_ds = normal_ds.map(lambda x: process_path(x,labels['normal']))\n",
    "pneumonia_ds = pneumonia_ds.map(lambda x: process_path(x,labels['pneumonia']))"
   ],
   "id": "d31815e9-4554-430c-b604-3a98e57e6d7d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the chest X-ray images of each class."
   ],
   "id": "fdf89e4b-d70f-4dc1-9ab3-2645d32c0bc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.load_img(covid_paths[5], color_mode='grayscale', target_size=(224,224))"
   ],
   "id": "fed55635-a3bf-42dc-9bb0-9444753c9407"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.load_img(normal_paths[5], color_mode='grayscale', target_size=(224,224))"
   ],
   "id": "ffd647fd-d608-4c5c-97f2-52d0ce61b975"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.load_img(pneumonia_paths[5], color_mode='grayscale', target_size=(224,224))"
   ],
   "id": "6099d982-2e4d-44f6-a9e2-7c7bbd3ec52a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the convolutional neural network(VGG19) via Transfer Learning\n",
    "\n",
    "In this section, we will train and evaluate our Convolutional Neural\n",
    "Network following the methodology outlined in the paper:\n",
    "\n",
    "-   Split the datasets into training, test, and validation sets.\n",
    "-   Apply the data augmentation strategy.\n",
    "-   Load the VGG19 model to extract features from the images.\n",
    "-   Add two dense layers with ReLU activation.\n",
    "-   Add a final dense layer with softmax activation.\n",
    "-   Train the model for 50 epochs with a learning rate of 0.001 using\n",
    "    the RMSprop optimizer.\n",
    "-   Use ReduceLROnPlateau to adjust the learning rate when validation\n",
    "    loss has stopped improving.\n",
    "-   Report the model’s accuracy, confusion matrix, and class-wise\n",
    "    precision, recall, and F1-score."
   ],
   "id": "aaf22fec-aa0a-41bd-afc3-d13ed70feed6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by splitting `covid_ds`, `normal_ds` and `pneumonia_ds`\n",
    "according to the statistics given in the paper. We then concatenate\n",
    "these splits to form training, test, and validation sets for model\n",
    "training and evaluation. `keras.applications.vgg19.preprocess_input`\n",
    "method is applied to preprocess the images, ensuring they are in the\n",
    "correct format required by the VGG19 model."
   ],
   "id": "6ec51ac6-10e6-4610-894e-8351f4c37b1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting normal patients data acc. to stats given in paper\n",
    "normal_ds_train = normal_ds.take(200)\n",
    "normal_remaining = normal_ds.skip(200)\n",
    "normal_ds_val = normal_remaining.take(50)\n",
    "normal_ds_test = normal_remaining.skip(50)\n",
    "# Splitting pneumonia patients data acc. to stats given in paper\n",
    "pneumonia_ds_train = pneumonia_ds.take(200)\n",
    "pneumonia_remaining = pneumonia_ds.skip(200)\n",
    "pneumonia_ds_val = pneumonia_remaining.take(50)\n",
    "pneumonia_ds_test = pneumonia_remaining.skip(50)\n",
    "# Splitting covid patients data acc. to stats given in paper\n",
    "covid_ds_train = covid_ds.take(180)\n",
    "covid_remaining = covid_ds.skip(180)\n",
    "covid_ds_val = covid_remaining.take(40)\n",
    "covid_ds_test = covid_remaining.skip(40)"
   ],
   "id": "5221d7ee-fea7-47ce-96a1-cd40de4d25be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "train_ds = (covid_ds_train.concatenate(normal_ds_train).concatenate(pneumonia_ds_train))\n",
    "validation_ds = (covid_ds_val.concatenate(normal_ds_val).concatenate(pneumonia_ds_val))\n",
    "test_ds = (covid_ds_test.concatenate(normal_ds_test).concatenate(pneumonia_ds_test))\n",
    "# Preprocess the images in each dataset using VGG19 preprocess_input\n",
    "train_ds = train_ds.map(lambda x, y: (keras.applications.vgg19.preprocess_input(x), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (keras.applications.vgg19.preprocess_input(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (keras.applications.vgg19.preprocess_input(x), y))"
   ],
   "id": "0a2afe8a-462b-40ab-b58a-d59516bf35e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we create `RandomRotation` , `RandomTranslation`,\n",
    "`RandomShear`, `RandomZoom` and `RandomFlip` data augmentation layers\n",
    "and apply them on the training set."
   ],
   "id": "58e68da6-5392-4421-9aa9-353bf4284162"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation layers\n",
    "augmentation_layers = [\n",
    "    layers.RandomRotation(0.2), # Randomly rotate images by up to 20 degrees\n",
    "    layers.RandomTranslation(0.1, 0.1), # Randomly translate images by up to 10% in x and y directions\n",
    "    layers_cv.RandomShear(x_factor=0.1, y_factor=0.1), # Randomly shear images by up to 10% in x and y directions\n",
    "    layers.RandomZoom(0.2), # Randomly zoom images by up to 20%\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\") # Randomly flip images horizontally and vertically\n",
    "]\n",
    "\n",
    "def data_augmentation(x):\n",
    "    for layer in augmentation_layers:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "# Apply data augmentation to training set\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))"
   ],
   "id": "171edb31-4db4-4531-a91f-ae181cbc135b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell batches the datasets (`train_ds`, `validation_ds`, and\n",
    "`test_ds`) into batches of 32 samples, uses prefetching to improve\n",
    "performance, and caches the datasets in memory for faster subsequent\n",
    "access."
   ],
   "id": "1c4f9f87-8116-414b-9ccb-96b52bada1a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Configure datasets for performance\n",
    "train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()\n",
    "validation_ds = validation_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()\n",
    "test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()"
   ],
   "id": "daae5fa9-b844-4dac-a696-c965c66963ab"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the VGG19 model with weights pretrained on the\n",
    "ImageNet dataset. By setting `include_top=False`, we exclude the final\n",
    "classification layer of the VGG19 model. We set the `trainable`\n",
    "attribute of the VGG19 layers to `False`. Then, we add a `Flatten` layer\n",
    "to convert the VGG19 output into a one-dimensional vector. We follow\n",
    "this with two `Dense` layers with ReLU activation, having 1024 and 512\n",
    "neurons respectively. Finally, the output is fed into a `Dense` layer\n",
    "with a softmax activation function."
   ],
   "id": "64bba049-c748-42b0-b63a-063c11591d74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base model using VGG19 pre-trained on ImageNet\n",
    "base_model = keras.applications.VGG19(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet\n",
    "    input_shape=[224, 224, 3],  # Specify input shape\n",
    "    include_top=False,  # Do not include the ImageNet classifier at the top\n",
    ")\n",
    "\n",
    "# Freeze the base model to prevent its weights from being updated during training\n",
    "base_model.trainable = False\n",
    "\n",
    "# Flatten the output of VGG19\n",
    "x = layers.Flatten()(base_model.output)\n",
    "\n",
    "# Add Dense layer with 1024 neurons and ReLU activation\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Add Dense layer with 512 neurons and ReLU activation\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add final Dense layer with softmax activation for 3-class prediction\n",
    "predictions = layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Create the full model\n",
    "model = keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Display model summary, showing which layers are trainable\n",
    "model.summary(show_trainable=True)"
   ],
   "id": "775ab254-0c73-4e4f-bb92-22a592bba3bd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on the training set using the RMSprop optimizer for\n",
    "50 epochs. We use `sparse_categorical_crossentropy` as our loss function\n",
    "since the labels are encoded as integers. We apply `ReduceLROnPlateau`\n",
    "to reduce the learning rate by a factor of 0.3 when the validation loss\n",
    "plateaus."
   ],
   "id": "b94d361f-6565-43ea-8426-1aee20f9b532"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# Define learning rate reduction callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(factor=0.3)\n",
    "\n",
    "# Set number of training epochs\n",
    "epochs = 50\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=validation_ds, callbacks=[reduce_lr])"
   ],
   "id": "3c238951-7654-47b1-9a25-eec33ab09b6b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the training accuracy (`sparse_categorical_accuracy`) and\n",
    "validation accuracy (`val_sparse_categorical_accuracy`) against the\n",
    "number of epochs."
   ],
   "id": "4ade6631-8533-4388-8287-305f61c9559b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "45ab96f2-500f-429d-9a86-5e2ec41f3c36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we evaluate the model on the test set and report the accuracy."
   ],
   "id": "82c18abc-c996-44d9-b034-5f2166cc5bb4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print('Test loss :', loss)\n",
    "print('Test accuracy :', accuracy)"
   ],
   "id": "9a219774-d7ae-421e-bf3a-c4c557234c30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cells display the true and predicted labels on the\n",
    "test set and generate a confusion matrix."
   ],
   "id": "87c373ba-14cb-4560-b644-fce792a0e1fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = model.predict(test_ds)\n",
    "# Convert predicted probabilities to class labels by taking the index of the highest probability\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Predictions:\",y_pred)\n",
    "# Extract true labels from the test dataset\n",
    "y_true = tf.concat([label for _, label in test_ds], axis=0).numpy()\n",
    "print(\"True labels:\", y_true)"
   ],
   "id": "925641b4-0106-4414-a4f6-98b16327547b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_mat = confusion_matrix(y_true,y_pred)\n",
    "# Display confusion matrix with labels\n",
    "ConfusionMatrixDisplay(conf_mat,display_labels=labels.keys()).plot(cmap='Blues')"
   ],
   "id": "df84e553-6696-45db-ad20-39868e3b8159"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we report the class-wise precision, recall and f1-score of the\n",
    "model’s performance on the test set."
   ],
   "id": "ce844830-6a66-470c-b6e2-27ebc5c37047"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# Print metrics for each class\n",
    "for key in labels.keys():\n",
    "  print(\"class:\", key)\n",
    "  print(\"Precision:\",report[str(labels[key])]['precision'])\n",
    "  print(\"Recall:\",report[str(labels[key])]['recall'])\n",
    "  print(\"F1-score:\",report[str(labels[key])]['f1-score'])\n",
    "  print()"
   ],
   "id": "eb092d8e-8b87-4848-a44d-9f719def8709"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s save the model for future inference tasks."
   ],
   "id": "3e378e8f-bfd3-47f6-8a71-36b6ad764a3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('correct_covid.keras')"
   ],
   "id": "8caa61b9-c93a-46f6-bd71-6a382c7b3959"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "|  Metric  | Original | Reproduced | Reproduced without Data Leakage |\n",
    "|:--------:|:--------:|:----------:|:-------------------------------:|\n",
    "| Accuracy |   89.3   |   92.14    |              51.43              |"
   ],
   "id": "447bd41b-d9ad-4532-8a8b-78dc12e81367"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1\\]: Rahaman, Md Mamunur et al. “Identification of COVID-19 samples\n",
    "from chest X-Ray images using deep learning: A comparison of transfer\n",
    "learning approaches.” Journal of X-ray science and technology vol. 28,5\n",
    "(2020): 821-839. doi:10.3233/XST-200715\n",
    "\n",
    "\\[3\\]: X. Wang, et al., “ChestX-Ray8: Hospital-Scale Chest X-Ray\n",
    "Database and Benchmarks on Weakly-Supervised Classification and\n",
    "Localization of Common Thorax Diseases,” in 2017 IEEE Conference on\n",
    "Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 2017\n",
    "pp. 3462-3471. doi: 10.1109/CVPR.2017.369"
   ],
   "id": "17cf8df2-561f-4cb1-92ec-59a196c80521"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
